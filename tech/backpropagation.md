---
{
"title":"역전파에 대해서 공부한 뒤",
"tags":["deep","learning"]
}
---

이 글을 쓰는 이유는 내가 알잘딱딸센으로 이야기한 역전파에 대한 이야기이다.

물론 틀린 내용도 있겠지만, 그래도 내가 역전파를 공부하면서 쾌변한 느낌을 받았던 방식이다.
(틀린 것이 있다면 자유롭게 이야기 부탁드립니다.)

우선 미분 대한 이야기를 하고 넘어가야된다고 생각한다.
미분이란 뭘까?
> 사실 나도 그딴거 잘 모른다.

고등학교 때 수학 푸는 것에 대한 관심이 없었고, 단지 이론만 듣는 것을 좋아했던 나에게는 미분이란 단지 극점을 구하는 방식으로만 알고 있다.
물론 추가적인 내용들이 더 있겠지만, 나에게는 그렇게 다가왔고 지금도 그렇게 기억하고 있다.

극점 혹은 도함수의 그래프의 함수값이 0인 지점을 의미하지만 나는 역전파를 공부하면서 살짝 바꾸어 이해하였다.

극점을 어느 특정 함수의 최저값으로 이해를 하고 쭉 봤더니 이해하기 쉬워서 그렇게 했다.

그러면 왜 미분을 이야기했냐? 

역전파라는 것은 결국 인공지능 혹은 딥러닝한 모델이 결과를 도출한 값과 실제 값의 차를 줄여나가는 과정이기 때문이다.
생각을 해보면 당연한 이야기이다. 실제 값과 가까워 질 수록 그에 대한 차는 점점 작아져서 0에 근접할 것이다.( 무한대로 가면 0으로 수렴히지만 컴퓨터의 한계와 기간의 한계로 0에 근접하게 된다.)

그렇기 때문에 에러 함수에 대해 미분을 하여 최소값으로 항하는 값을 구하는 것이다.

여기서 에러 함수와 다양한 이야기를 더 하고 넘어가야 한다. 

에러 함수는 딥러닝 모델 혹은 퍼셉트론 모델이 도출한 결과와 실제값의 차를 함수로 도출하는 함수이다.

$$
E = \sum_j{1 \over 2}(t_j - o_i)^2
$$

이게 에러함수의 모습이다. 

$t_j$는 실제 값을 $o_j$는 모델에서 도출한 결과 값이다.

여기서 우리가 봐야할 사실은 $o_j$의 값을 제외하고 다 아는 값이라는 것이다. 
이 사실을 기억해 두고 넘어가보자

2번째는 활성화 함수이다.
활성화 함수도 정말 다양하게 많은 함수들이 존재하지만 결국 사용하는 함수가 정해져 있고 가장 대표적인 함수 sigmoid를 보면

$$
o_j = {1 \over (1 - e^{-net_j}))}
$$

의 형태를 띄기에 마찬가지로 $o_j$를 $f(x)$로 바꿔서 본다면 결국 이 또한 모르는 미지수는 $net_j$으로 되기 때문이다.
